from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from typing import Dict, List
import json
from config.settings import LLM_MODEL, LLM_TEMPERATURE, OPENAI_API_KEY
from tools.evaluation_tools import EvaluationTools
from prompts.improvement import IMPROVEMENT_SUGGESTION_PROMPT, COMPARISON_PROMPT

class ImprovementAdvisor:
    """ê°œì„ ì•ˆ ì œì•ˆ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            openai_api_key=OPENAI_API_KEY
        )
        self.eval_tools = EvaluationTools()
    
    def suggest_improvements(
        self,
        service_name: str,
        risk_assessment: Dict
    ) -> List[Dict]:
        """
        ë¦¬ìŠ¤í¬ í‰ê°€ ê¸°ë°˜ ê°œì„ ì•ˆ ì œì•ˆ
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
            risk_assessment: ë¦¬ìŠ¤í¬ í‰ê°€ ê²°ê³¼
            
        Returns:
            ê°œì„ ì•ˆ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n{'='*50}")
        print(f"ğŸ’¡ [{service_name}] ê°œì„ ì•ˆ ì œì•ˆ ìƒì„± ì¤‘...")
        print(f"{'='*50}\n")
        
        # 1. ê°œì„  ìš°ì„ ìˆœìœ„ íŒŒì•…
        priority_areas = self.eval_tools.prioritize_improvements(risk_assessment)
        
        if not priority_areas:
            print(f"  âœ… ì¦‰ê°ì ì¸ ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"  ğŸ“Œ ê°œì„  í•„ìš” ì˜ì—­: {len(priority_areas)}ê°œ")
        for area in priority_areas:
            print(f"     - {area['dimension']}: {area['current_score']}/5 (ìš°ì„ ìˆœìœ„: {area['priority']})")
        
        # 2. LLMì„ í†µí•œ ê°œì„ ì•ˆ ìƒì„±
        improvements = self._generate_improvements(
            service_name=service_name,
            risk_assessment=risk_assessment,
            priority_areas=priority_areas
        )
        
        print(f"\n  âœ… ê°œì„ ì•ˆ ìƒì„± ì™„ë£Œ!")
        print(f"{'='*50}\n")
        
        return improvements
    
    def _generate_improvements(
        self,
        service_name: str,
        risk_assessment: Dict,
        priority_areas: List[Dict]
    ) -> List[Dict]:
        """LLMì„ í†µí•œ ê°œì„ ì•ˆ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = IMPROVEMENT_SUGGESTION_PROMPT.format(
            service_name=service_name,
            risk_assessment=json.dumps(risk_assessment, ensure_ascii=False, indent=2),
            priority_areas=json.dumps(priority_areas, ensure_ascii=False, indent=2)
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ]
        
        response = self.llm.invoke(messages)
        
        # JSON íŒŒì‹±
        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            
            improvements = json.loads(content.strip())
            return improvements
        
        except json.JSONDecodeError as e:
            print(f"  âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
    
    def compare_services(
        self,
        services_data: Dict[str, Dict]
    ) -> str:
        """
        ì—¬ëŸ¬ ì„œë¹„ìŠ¤ ë¹„êµ ë¶„ì„
        
        Args:
            services_data: {service_name: {analysis, risk_assessment, improvements}}
            
        Returns:
            ë¹„êµ ë¶„ì„ í…ìŠ¤íŠ¸
        """
        if len(services_data) < 2:
            return ""
        
        print(f"\n{'='*50}")
        print(f"ğŸ“Š ì„œë¹„ìŠ¤ ë¹„êµ ë¶„ì„ ì¤‘...")
        print(f"{'='*50}\n")
        
        # ì„œë¹„ìŠ¤ ë¦¬ìŠ¤íŠ¸
        services_list = ", ".join(services_data.keys())
        
        # ê° ì„œë¹„ìŠ¤ì˜ ë¦¬ìŠ¤í¬ í‰ê°€ ê²°ê³¼ ì¶”ì¶œ
        all_assessments = {
            name: data['risk_assessment']
            for name, data in services_data.items()
        }
        
        # í‰ê°€ ë„êµ¬ë¡œ ë¹„êµ
        comparison_data = self.eval_tools.compare_services(all_assessments)
        
        # LLMì„ í†µí•œ ë¹„êµ ë¶„ì„ ìƒì„±
        prompt = COMPARISON_PROMPT.format(
            services_list=services_list,
            all_assessments=json.dumps(all_assessments, ensure_ascii=False, indent=2)
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ AI ì„œë¹„ìŠ¤ ë¹„êµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ]
        
        response = self.llm.invoke(messages)
        
        print(f"  âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
        print(f"{'='*50}\n")
        
        return response.content
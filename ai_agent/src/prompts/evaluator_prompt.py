RISK_EVALUATOR_PROMPT = """당신은 국제 가이드라인을 기반으로 리스크를 평가하는 AI 윤리 전문가입니다.

서비스 분석:
{service_analysis}

관련 가이드라인:
{guidelines}

이 AI 서비스의 {risk_category} 리스크를 평가하세요.

다음을 고려하세요:
1. EU AI Act, UNESCO, OECD 가이드라인의 구체적 요구사항
2. 잠재적 피해 및 위반사항
3. 리스크의 심각도 및 발생 가능성
4. 영향받는 이해관계자

다음 JSON 형식으로 평가를 제공하세요:
{{
    "리스크_점수": <0-100>,
    "리스크_수준": "<없음|낮음|중간|높음|매우높음>",
    "발견사항": [
        {{
            "이슈": "<구체적 이슈>",
            "심각도": "<낮음|중간|높음|매우높음>",
            "가이드라인_참조": "<어떤 가이드라인>",
            "증거": "<서비스 분석에서의 증거>",
            "잠재적_피해": "<피해 설명>"
        }}
    ],
    "규정_갭": ["<규정 준수 갭 목록>"],
    "주요_우려사항": ["<우선순위화된 우려사항>"]
}}

구체적으로 작성하고 관련 가이드라인을 인용하세요. 모든 내용은 한국어로 작성하세요.
"""


def get_evaluator_prompt(service_analysis: dict, guidelines: list, risk_category: str) -> str:
    guidelines_
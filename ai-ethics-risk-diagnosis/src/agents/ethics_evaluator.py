"""
ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ í‰ê°€ ì—ì´ì „íŠ¸
"""
import json
from typing import Dict, List
from langchain_openai import ChatOpenAI
from src.state import EthicsRiskState
from src.tools import WebSearchTool, RAGRetriever
from src.tools import calculate_risk_level, calculate_weighted_score
from src.prompts import get_ethics_evaluation_prompt
from src.config import LLM_MODEL, LLM_TEMPERATURE, ETHICS_CRITERIA


class EthicsEvaluatorAgent:
    """AI ìœ¤ë¦¬ì„± í‰ê°€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, rag_retriever: RAGRetriever):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE
        )
        self.web_search = WebSearchTool()
        self.rag_retriever = rag_retriever
    
    def evaluate(self, state: EthicsRiskState) -> EthicsRiskState:
        """
        ìœ¤ë¦¬ì„± ë¦¬ìŠ¤í¬ í‰ê°€ ìˆ˜í–‰
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
        """
        print("\n" + "="*50)
        print("âš–ï¸ STEP 2: Ethics Evaluation")
        print("="*50)
        
        service_name = state["target_service"]
        service_overview = state.get("service_overview", {})
        
        if not service_overview:
            state["errors"].append("Service overview not found")
            return state
        
        try:
            ethics_evaluation = {}
            criterion_scores = {}
            
            # ê° ìœ¤ë¦¬ ê¸°ì¤€ë³„ í‰ê°€
            for criterion, criterion_info in ETHICS_CRITERIA.items():
                print(f"\nğŸ“Š Evaluating: {criterion_info['name']}")
                
                # 1. ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
                print(f"   ğŸ“š Retrieving guidelines for {criterion}...")
                guidelines = self.rag_retriever.retrieve_for_criterion(
                    criterion,
                    service_context=service_overview.get('description', '')
                )
                
                # 2. ì›¹ ê²€ìƒ‰
                print(f"   ğŸŒ Searching ethics information...")
                web_results = self.web_search.search_ethics_info(service_name, criterion)
                
                # 3. LLM í‰ê°€
                print(f"   ğŸ¤– Analyzing with LLM...")
                prompt = get_ethics_evaluation_prompt(
                    criterion=criterion,
                    criterion_info=criterion_info,
                    service_overview=service_overview,
                    guidelines=guidelines,
                    web_search_results=web_results
                )
                
                response = self.llm.invoke(prompt)
                eval_text = response.content
                
                # JSON íŒŒì‹±
                if "```json" in eval_text:
                    eval_text = eval_text.split("```json")[1].split("```")[0].strip()
                elif "```" in eval_text:
                    eval_text = eval_text.split("```")[1].split("```")[0].strip()
                
                eval_result = json.loads(eval_text)
                
                # ë¦¬ìŠ¤í¬ ë ˆë²¨ ê³„ì‚°
                score = eval_result.get("score", 5)
                eval_result["risk_level"] = calculate_risk_level(score)
                
                ethics_evaluation[criterion] = eval_result
                criterion_scores[criterion] = score
                
                print(f"   âœ… Score: {score}/10 ({eval_result['risk_level']})")
                
                # ì°¸ì¡° ë¬¸ì„œì— ê°€ì´ë“œë¼ì¸ ì¶”ê°€
                if guidelines and state.get("references"):
                    for guide in guidelines[:1]:  # ê° ê¸°ì¤€ë‹¹ 1ê°œë§Œ
                        state["references"].append({
                            "source": guide["source"],
                            "section": f"Related to {criterion_info['name']}",
                            "content": guide["content"][:200]
                        })
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            overall_score = calculate_weighted_score(criterion_scores)
            overall_risk_level = calculate_risk_level(overall_score)
            
            ethics_evaluation["overall_score"] = overall_score
            ethics_evaluation["overall_risk_level"] = overall_risk_level
            
            print(f"\n{'='*50}")
            print(f"ğŸ“ˆ Overall Score: {overall_score}/10")
            print(f"âš ï¸  Overall Risk Level: {overall_risk_level}")
            print(f"{'='*50}")
            
            # State ì—…ë°ì´íŠ¸
            state["ethics_evaluation"] = ethics_evaluation
            state["current_step"] = "ethics_evaluation_completed"
            state["messages"].append({
                "role": "assistant",
                "content": f"Ethics evaluation completed. Overall score: {overall_score}/10"
            })
            
        except Exception as e:
            error_msg = f"Ethics evaluation failed: {str(e)}"
            print(f"\nâŒ {error_msg}")
            state["errors"].append(error_msg)
            state["current_step"] = "ethics_evaluation_failed"
        
        return state


def ethics_evaluator_node(state: EthicsRiskState, rag_retriever: RAGRetriever) -> EthicsRiskState:
    """ìœ¤ë¦¬ í‰ê°€ ë…¸ë“œ"""
    agent = EthicsEvaluatorAgent(rag_retriever)
    return agent.evaluate(state)
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from typing import Dict, List
import json
from config.settings import LLM_MODEL, LLM_TEMPERATURE, OPENAI_API_KEY, ETHICS_GUIDELINES
from tools.rag_tools import RAGTools
from tools.search_tools import SearchTools
from tools.evaluation_tools import EvaluationTools
from prompts.risk_assessment import RISK_ASSESSMENT_PROMPT

class RiskAssessor:
    """ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ - í¸í–¥ì„±, í”„ë¼ì´ë²„ì‹œ, íˆ¬ëª…ì„± ë“± í‰ê°€"""
    
    def __init__(self, rag_tools: RAGTools):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            openai_api_key=OPENAI_API_KEY
        )
        self.rag_tools = rag_tools
        self.search_tools = SearchTools()
        self.eval_tools = EvaluationTools()
        self.criteria = self.eval_tools.load_ethics_criteria()
    
    def assess_risks(
        self, 
        service_name: str, 
        service_analysis: Dict
    ) -> Dict:
        """ì„œë¹„ìŠ¤ì˜ ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì¢…í•© í‰ê°€"""
        
        print(f"\n{'='*60}")
        print(f"âš–ï¸  [{service_name}] ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì‹œì‘")
        print(f"{'='*60}\n")
        
        risk_assessment = {}
        
        # ê° ìœ¤ë¦¬ ì°¨ì›ë³„ë¡œ í‰ê°€
        for dimension, config in self.criteria.items():
            print(f"  ğŸ“Š {config['name']} í‰ê°€ ì¤‘...")
            
            assessment = self._assess_dimension(
                service_name=service_name,
                service_analysis=service_analysis,
                dimension=dimension,
                dimension_config=config
            )
            
            risk_assessment[dimension] = assessment
            
            # ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²°ê³¼ í‘œì‹œ
            auto_check = assessment.get('automated_checks', {})
            print(f"     â†’ LLM ì ìˆ˜: {assessment['score']}/5 ({assessment['risk_level']})")
            print(f"     â†’ ìë™ì²´í¬: {auto_check.get('passed_checks', 0)}/{auto_check.get('total_checks', 0)} í†µê³¼")
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        scores = {dim: data['score'] for dim, data in risk_assessment.items()}
        overall_score = self.eval_tools.calculate_overall_score(scores)
        overall_risk = self.eval_tools.get_risk_level(overall_score)
        
        risk_assessment['overall_score'] = overall_score
        risk_assessment['overall_risk_level'] = overall_risk
        
        print(f"\n  âœ… ì¢…í•© í‰ê°€: {overall_score}/5 (ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: {overall_risk})")
        print(f"{'='*60}\n")
        
        return risk_assessment
    
    def _assess_dimension(
        self,
        service_name: str,
        service_analysis: Dict,
        dimension: str,
        dimension_config: Dict
    ) -> Dict:
        """íŠ¹ì • ìœ¤ë¦¬ ì°¨ì›ì— ëŒ€í•œ í‰ê°€"""
        
        # 1. ìë™ ì²´í¬ë¦¬ìŠ¤íŠ¸ í‰ê°€
        checklist_result = self.eval_tools.automated_checklist_evaluation(
            service_analysis, dimension
        )
        
        # 2. RAGë¡œ ê°€ì´ë“œë¼ì¸ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        guideline_context = self.rag_tools.get_guideline_context(
            dimension=dimension,
            guidelines=ETHICS_GUIDELINES
        )
        
        # 3. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
        search_results = self.search_tools.search_service_info(
            service_name=service_name,
            query_type=dimension
        )
        
        search_context = "\n\n".join([
            f"[{r['title']}]\n{r['content'][:500]}..."
            for r in search_results[:2]
        ]) if search_results else "ì¶”ê°€ ê²€ìƒ‰ ì •ë³´ ì—†ìŒ"
        
        # 4. í‰ê°€ ê¸°ì¤€ ìƒì„±
        evaluation_criteria = self._format_evaluation_criteria(dimension_config)
        
        # 5. LLM í‰ê°€
        prompt = RISK_ASSESSMENT_PROMPT.format(
            service_name=service_name,
            service_analysis=json.dumps(service_analysis, ensure_ascii=False, indent=2),
            dimension=dimension,
            dimension_description=dimension_config['description'],
            guideline_context=guideline_context,
            search_context=search_context,
            evaluation_criteria=evaluation_criteria
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. EU AI Act, UNESCO, OECD ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ê´€ì ì´ê³  ê·¼ê±° ìˆëŠ” í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”."),
            HumanMessage(content=prompt)
        ]
        
        try:
            response = self.llm.invoke(messages)
            assessment = self._parse_and_validate_assessment(response.content)
            
            # ë¦¬ìŠ¤í¬ ë ˆë²¨ ê³„ì‚°
            assessment['risk_level'] = self.eval_tools.get_risk_level(assessment['score'])
            assessment['automated_checks'] = checklist_result
            
            # LLM vs ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìˆ˜ ë¹„êµ
            score_diff = abs(assessment['score'] - checklist_result['checklist_score'])
            if score_diff > 2:
                print(f"     âš ï¸  í‰ê°€ ì ìˆ˜ ì°¨ì´ í¼ (LLM: {assessment['score']}, ìë™: {checklist_result['checklist_score']:.1f})")
            
            return assessment
            
        except Exception as e:
            print(f"    âš ï¸  í‰ê°€ ì˜¤ë¥˜: {e}")
            return self._get_default_assessment(dimension, checklist_result)
    
    def _format_evaluation_criteria(self, config: Dict) -> str:
        """í‰ê°€ ê¸°ì¤€ í¬ë§·íŒ…"""
        criteria_text = f"{config['name']}\n\ní‰ê°€ í•­ëª©:\n"
        
        for i, point in enumerate(config.get('evaluation_points', []), 1):
            criteria_text += f"{i}. {point}\n"
        
        return criteria_text
    
    def _parse_and_validate_assessment(self, content: str) -> Dict:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹± ë° ê²€ì¦"""
        
        # JSON ì¶”ì¶œ
        content = content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        
        assessment = json.loads(content.strip())
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required = ['score', 'description', 'evidence', 'guideline_compliance', 'reasoning']
        for field in required:
            if field not in assessment:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        # ì ìˆ˜ ê²€ì¦ ë° ì •ê·œí™”
        score = assessment['score']
        if not isinstance(score, (int, float)) or score < 1 or score > 5:
            raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì ìˆ˜: {score}")
        
        assessment['score'] = int(round(score))
        
        # í’ˆì§ˆ ê²€ì¦
        if len(assessment['evidence']) < 2:
            print("    âš ï¸  ì¦ê±° ë¶€ì¡±")
        
        if len(assessment['description']) < 100:
            print("    âš ï¸  ì„¤ëª… ë¶€ì¡±")
        
        return assessment
    
    def _get_default_assessment(self, dimension: str, checklist_result: Dict) -> Dict:
        """ê¸°ë³¸ í‰ê°€ ê²°ê³¼"""
        
        score = 3
        if checklist_result:
            score = max(1, min(5, int(round(checklist_result['checklist_score']))))
        
        return {
            "score": score,
            "risk_level": self.eval_tools.get_risk_level(score),
            "description": f"{dimension} í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ í‰ê°€ ì‚¬ìš©.",
            "evidence": ["ìë™ í‰ê°€", "ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜"],
            "guideline_compliance": {
                "EU AI Act": "í™•ì¸ ë¶ˆê°€",
                "UNESCO": "í™•ì¸ ë¶ˆê°€",
                "OECD": "í™•ì¸ ë¶ˆê°€"
            },
            "reasoning": "í‰ê°€ ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ê°’",
            "risks_identified": ["í‰ê°€ ë¶ˆê°€"],
            "strengths": [],
            "automated_checks": checklist_result
        }

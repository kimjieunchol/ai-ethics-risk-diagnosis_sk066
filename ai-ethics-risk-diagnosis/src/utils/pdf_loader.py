"""
PDF ë¬¸ì„œ ë¡œë”© ìœ í‹¸ë¦¬í‹°
"""
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document


def load_pdf_documents(pdf_paths: List[str], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:
    """
    PDF ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
    
    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    all_documents = []
    
    for pdf_path in pdf_paths:
        try:
            # PDF ë¡œë“œ
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in documents:
                doc.metadata["source_file"] = pdf_path.split("/")[-1]
            
            all_documents.extend(documents)
            print(f"âœ… Loaded: {pdf_path} ({len(documents)} pages)")
            
        except Exception as e:
            print(f"âŒ Error loading {pdf_path}: {e}")
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = text_splitter.split_documents(all_documents)
    print(f"ğŸ“„ Total chunks created: {len(chunks)}")
    
    return chunks
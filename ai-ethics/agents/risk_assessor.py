from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from typing import Dict, List
import json
from config.settings import (
    LLM_MODEL, 
    LLM_TEMPERATURE, 
    OPENAI_API_KEY,
    ETHICS_DIMENSIONS
)
from tools.rag_tools import RAGTools
from tools.search_tools import SearchTools
from tools.evaluation_tools import EvaluationTools
from prompts.risk_assessment import (
    RISK_ASSESSMENT_PROMPT,
    DIMENSION_CRITERIA_MAP
)

class RiskAssessor:
    """ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸"""
    
    def __init__(self, rag_tools: RAGTools):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            openai_api_key=OPENAI_API_KEY
        )
        self.rag_tools = rag_tools
        self.search_tools = SearchTools()
        self.eval_tools = EvaluationTools()
    
    def assess_risks(
        self, 
        service_name: str, 
        service_analysis: Dict,
        guidelines: List[str]
    ) -> Dict:
        """
        ì„œë¹„ìŠ¤ì˜ ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì¢…í•© í‰ê°€
        
        Args:
            service_name: ì„œë¹„ìŠ¤ëª…
            service_analysis: ì„œë¹„ìŠ¤ ë¶„ì„ ê²°ê³¼
            guidelines: í‰ê°€ ê¸°ì¤€ ê°€ì´ë“œë¼ì¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì°¨ì›ë³„ ë¦¬ìŠ¤í¬ í‰ê°€ ê²°ê³¼
        """
        print(f"\n{'='*50}")
        print(f"âš–ï¸  [{service_name}] ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ í‰ê°€ ì‹œì‘...")
        print(f"{'='*50}\n")
        
        risk_assessment = {}
        
        # ê° ìœ¤ë¦¬ ì°¨ì›ë³„ë¡œ í‰ê°€
        for dimension, config in ETHICS_DIMENSIONS.items():
            print(f"  ğŸ“Š {dimension}({config['description']}) í‰ê°€ ì¤‘...")
            
            assessment = self._assess_dimension(
                service_name=service_name,
                service_analysis=service_analysis,
                dimension=dimension,
                dimension_description=config['description'],
                guidelines=guidelines
            )
            
            risk_assessment[dimension] = assessment
            print(f"     â†’ ì ìˆ˜: {assessment['score']}/5 ({assessment['risk_level']})")
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        scores = {dim: data['score'] for dim, data in risk_assessment.items()}
        overall_score = self.eval_tools.calculate_overall_score(scores)
        risk_assessment['overall_score'] = overall_score
        
        print(f"\n  âœ… ì¢…í•© ì ìˆ˜: {overall_score}/5")
        print(f"{'='*50}\n")
        
        return risk_assessment
    
    def _assess_dimension(
        self,
        service_name: str,
        service_analysis: Dict,
        dimension: str,
        dimension_description: str,
        guidelines: List[str]
    ) -> Dict:
        """íŠ¹ì • ìœ¤ë¦¬ ì°¨ì›ì— ëŒ€í•œ í‰ê°€"""
        
        # 1. ê°€ì´ë“œë¼ì¸ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (RAG)
        guideline_context = self.rag_tools.get_guideline_context(
            dimension=dimension,
            guidelines=guidelines
        )
        
        # 2. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
        search_results = self.search_tools.search_service_info(
            service_name=service_name,
            query_type=dimension
        )
        
        search_context = "\n\n".join([
            f"[{r['title']}] {r['content'][:300]}..."
            for r in search_results[:2]
        ])
        
        # 3. í‰ê°€ ê¸°ì¤€
        evaluation_criteria = DIMENSION_CRITERIA_MAP.get(dimension, "")
        
        # 4. LLMì„ í†µí•œ í‰ê°€
        prompt = RISK_ASSESSMENT_PROMPT.format(
            service_name=service_name,
            service_analysis=json.dumps(service_analysis, ensure_ascii=False, indent=2),
            dimension=dimension,
            dimension_description=dimension_description,
            guideline_context=guideline_context + "\n\nì¶”ê°€ ì •ë³´:\n" + search_context,
            evaluation_criteria=evaluation_criteria
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ]
        
        response = self.llm.invoke(messages)
        
        # JSON íŒŒì‹±
        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            
            assessment = json.loads(content.strip())
            
            # ë¦¬ìŠ¤í¬ ë ˆë²¨ ìë™ ê³„ì‚° (ì¼ê´€ì„± ë³´ì¥)
            assessment['risk_level'] = self.eval_tools.get_risk_level(assessment['score'])
            
            return assessment
        
        except json.JSONDecodeError as e:
            print(f"    âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "score": 3,
                "risk_level": "ì¤‘ê°„",
                "description": "í‰ê°€ ì˜¤ë¥˜ ë°œìƒ",
                "evidence": [],
                "guideline_compliance": {}
            }